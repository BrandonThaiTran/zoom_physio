{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "from imutils.video import VideoStream, FileVideoStream\n",
    "from imutils import face_utils\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import dlib\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face streamer class \n",
    "class face_streamer:\n",
    "    def __init__(self, predictor_path, filename = None):\n",
    "        self.filename = filename\n",
    "        \n",
    "        # Initialize dlib's face detector (HOG-based)\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        \n",
    "        # Create landmark predictor.\n",
    "        self.predictor = dlib.shape_predictor(\"../detect-face-parts/shape_predictor_68_face_landmarks.dat\")\n",
    "        \n",
    "        # Facial landmarks that we use\n",
    "        facial_landmarks_idxs = OrderedDict([\n",
    "            (\"nose\", (27, 36)),\n",
    "            (\"face\", (0, 26))\n",
    "        ])\n",
    "        \n",
    "    def start_stream(self):\n",
    "        if self.filename:\n",
    "            self.vs = FileVideoStream(self.filename).start()\n",
    "        else:\n",
    "            self.vs = VideoStream(src=0).start()\n",
    "        print(\"[INFO] camera sensor warming up...\")\n",
    "        time.sleep(2.0)\n",
    "            \n",
    "    def end_stream(self):\n",
    "        # Do some cleanup\n",
    "        cv2.destroyAllWindows()\n",
    "        self.vs.stop()\n",
    "    \n",
    "    def stream(self, display_face_bb = False, display_landmarks = False):\n",
    "        self.start_stream()\n",
    "        # Loop and stream\n",
    "        while True:\n",
    "            # Read and resize the frame\n",
    "            frame = self.vs.read()\n",
    "            frame = imutils.resize(frame, width=400)\n",
    "            # Get grayscale image and extract the bounding boxes with the detector \n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            rects = self.detector(gray, 0)\n",
    "            # Loop over the face detections \n",
    "            for rect in rects:\n",
    "                # Display bounding box if true\n",
    "                if display_face_bb:\n",
    "                    frame = self.display_face_bb(frame, rect)\n",
    "                # Determine the facial landmarks for the face region, then\n",
    "                # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "                # array.\n",
    "                shape = face_utils.shape_to_np(self.predictor(gray, rect))\n",
    "                # Display facial landmarks if true\n",
    "                if display_landmarks:\n",
    "                    frame = self.display_landmarks(frame, rect)\n",
    "                \n",
    "            # Show the frame\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            \n",
    "            # If the `q` key was pressed, break from the loop.\n",
    "            if key == ord(\"q\"):\n",
    "                # Do some cleanup \n",
    "                self.end_stream()\n",
    "                del self.vs\n",
    "                break\n",
    "    \n",
    "    def display_face_bb(self, frame, rect):\n",
    "        # Compute the bounding box of the face and draw it on the\n",
    "        # frame\n",
    "        (bX, bY, bW, bH) = face_utils.rect_to_bb(rect)\n",
    "        cv2.rectangle(frame, (bX, bY), (bW+bX, bH+bY), (0, 255, 0), 1)\n",
    "        return frame\n",
    "    \n",
    "    def display_landmarks(self, frame, rect):\n",
    "        for (name, (i, j)) in self.facial_landmarks_idxs.items():\n",
    "            # Loop over the subset of facial landmarks, drawing the specific face part\n",
    "            for (x, y) in shape[i:j]:\n",
    "                cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "        return frame \n",
    "        \n",
    "\n",
    "    def draw_aam(self):\n",
    "        return\n",
    "                \n",
    "# next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] camera sensor warming up...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'face_streamer' object has no attribute 'facial_landmarks_idxs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9bbca2485766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictor_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../detect-face-parts/shape_predictor_68_face_landmarks.dat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_streamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_face_bb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_landmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# del fs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8ed1ccbc317c>\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, display_face_bb, display_landmarks)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;31m# Display facial landmarks if true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdisplay_landmarks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Show the frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8ed1ccbc317c>\u001b[0m in \u001b[0;36mdisplay_landmarks\u001b[0;34m(self, frame, rect)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfacial_landmarks_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Loop over the subset of facial landmarks, drawing the specific face part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'face_streamer' object has no attribute 'facial_landmarks_idxs'"
     ]
    }
   ],
   "source": [
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "    (\"nose\", (27, 36)),\n",
    "    (\"face\", (0, 26))\n",
    "])\n",
    "predictor_path = \"../detect-face-parts/shape_predictor_68_face_landmarks.dat\"\n",
    "fs = face_streamer(predictor_path)\n",
    "fs.stream(display_face_bb = False, display_landmarks = True)\n",
    "# del fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_face_detection(filename=None):\n",
    "    # To capture video from webcam.\n",
    "    if not _filename:\n",
    "        vs = VideoStream(src=0).start()\n",
    "    else:\n",
    "        # To use a video file as input \n",
    "        vs = FileVideoStream(filename).start()\n",
    "    time.sleep(2.0)\n",
    "\n",
    "    while True:\n",
    "        # grab frame and resize\n",
    "        frame = vs.read()\n",
    "        frame = imutils.resize(frame, width=400)\n",
    "        # grab the frame dimensions and convert it to a blob\n",
    "        (h, w) = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "            (300, 300), (104.0, 177.0, 123.0))\n",
    "        # pass the blob through the network and obtain the detections and\n",
    "        # predictions\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # extract the confidence (i.e., probability) associated with the\n",
    "            # prediction\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            # filter out weak detections by ensuring the `confidence` is\n",
    "            # greater than the minimum confidence\n",
    "            if confidence < confidence_threshold:\n",
    "                continue\n",
    "            # compute the (x, y)-coordinates of the bounding box for the\n",
    "            # object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # draw the bounding box of the face along with the associated\n",
    "            # probability\n",
    "            text = \"{:.2f}%\".format(confidence * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "            cv2.putText(frame, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            # do a bit of cleanup\n",
    "            cv2.destroyAllWindows()\n",
    "            vs.stop()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(_path):\n",
    "    img = cv2.imread(_path,1)\n",
    "    while True:\n",
    "        cv2.imshow('image',img)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            # do a bit of cleanup\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
