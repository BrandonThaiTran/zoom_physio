{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "from imutils.video import VideoStream, FileVideoStream\n",
    "from imutils import face_utils\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import dlib\n",
    "from collections import OrderedDict\n",
    "from pose_estimator import PoseEstimator\n",
    "from stabilizer import Stabilizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face streamer class \n",
    "class face_streamer:\n",
    "    def __init__(self, predictor_path, filename = None):\n",
    "        self.filename = filename\n",
    "        \n",
    "        # Initialize dlib's face detector (HOG-based)\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        \n",
    "        # Create landmark predictor.\n",
    "        self.predictor = dlib.shape_predictor(\"../detect-face-parts/shape_predictor_68_face_landmarks.dat\")\n",
    "        \n",
    "        # Facial landmarks that we use\n",
    "        self.facial_landmarks_idxs = OrderedDict([\n",
    "            (\"nose\", (27, 36)),\n",
    "            (\"face\", (0, 26))\n",
    "        ])\n",
    "        \n",
    "        # Define the pose estimator and stabilizer \n",
    "        self.height, self.width = 300, 400\n",
    "        self.pose_estimator = PoseEstimator(img_size=(self.height, self.width))\n",
    "        # Define scalar stabilizers for pose.\n",
    "        self.pose_stabilizers = [Stabilizer(state_num=2, measure_num=1, cov_process=0.1, cov_measure=0.1) \n",
    "                            for _ in range(6)]\n",
    "        \n",
    "        # RGB and RPY to track\n",
    "        self.red = []\n",
    "        self.green = []\n",
    "        self.blue = []\n",
    "        self.roll = []\n",
    "        self.pitch = []\n",
    "        self.yaw = []\n",
    "        \n",
    "        \n",
    "    def start_stream(self):\n",
    "        if self.filename:\n",
    "            self.vs = FileVideoStream(self.filename).start()\n",
    "        else:\n",
    "            self.vs = VideoStream(src=0).start()\n",
    "        print(\"[INFO] camera sensor warming up...\")\n",
    "        time.sleep(2.0)\n",
    "            \n",
    "    def end_stream(self):\n",
    "        # Do some cleanup\n",
    "        cv2.destroyAllWindows()\n",
    "        self.vs.stop()\n",
    "    \n",
    "    def stream(self, display_face_bb = False, display_landmarks = False, display_overlay = False):\n",
    "        self.start_stream()\n",
    "        # Loop and stream\n",
    "        while True:\n",
    "            # Read and resize the frame\n",
    "            frame = self.vs.read()\n",
    "            frame = imutils.resize(frame, width=400)\n",
    "            # Get grayscale image and extract the bounding boxes with the detector \n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            rects = self.detector(gray, 0)\n",
    "            # Loop over the face detections \n",
    "            for rect in rects:\n",
    "                # Get the bounding box \n",
    "                (bX, bY, bW, bH) = face_utils.rect_to_bb(rect)\n",
    "                # TODO -- take only the ROI, not the whole box \n",
    "                # Get RGB values\n",
    "                self.update_rgb(frame[bY:bH+bY, bX:bW+bX,:]) \n",
    "                # Determine the facial landmarks for the face region, then\n",
    "                # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "                # array.\n",
    "                shape = face_utils.shape_to_np(self.predictor(gray, rect))\n",
    "                # Try pose estimation\n",
    "                pose = self.pose_estimator.solve_pose_by_68_points(shape.astype('float'))\n",
    "                # Stabilize the pose\n",
    "                steady_pose = self.stablize_pose(pose)\n",
    "                # Display bounding box if true\n",
    "                if display_face_bb:\n",
    "                    frame = self.display_face_bb(frame, (bX, bY, bW, bH))\n",
    "                # Display facial landmarks if true\n",
    "                if display_landmarks:\n",
    "                    frame = self.display_landmarks(frame, shape)\n",
    "                # Display the landmark overlay if true\n",
    "                if display_overlay:\n",
    "                    frame = self.display_overlay(frame, shape)                \n",
    "                \n",
    "            # Show the frame\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            \n",
    "            # If the `q` key was pressed, break from the loop.\n",
    "            if key == ord(\"q\"):\n",
    "                # Do some cleanup \n",
    "                self.end_stream()\n",
    "                del self.vs\n",
    "                break\n",
    "                \n",
    "    # TODO -- only use the mask values, not just the box \n",
    "    def update_rgb(self, mask):\n",
    "        self.red.append(np.average(mask[:,:,0]) / 255)\n",
    "        self.green.append(np.average(mask[:,:,1]) / 255)\n",
    "        self.blue.append(np.average(mask[:,:,2]) / 255)\n",
    "                \n",
    "    def stablize_pose(self, pose):\n",
    "        steady_pose = []\n",
    "        pose_np = np.array(pose).flatten()\n",
    "        for value, ps_stb in zip(pose_np, self.pose_stabilizers):\n",
    "            ps_stb.update([value])\n",
    "            steady_pose.append(ps_stb.state[0])\n",
    "        steady_pose = np.reshape(steady_pose, (-1, 3))\n",
    "        return steady_pose \n",
    "    \n",
    "    def display_face_bb(self, frame, bounds):\n",
    "        (bX, bY, bW, bH) = bounds\n",
    "        # Draw the bounding box on the frame\n",
    "        cv2.rectangle(frame, (bX, bY), (bW+bX, bH+bY), (0, 255, 0), 1)\n",
    "        return frame\n",
    "    \n",
    "    def display_landmarks(self, frame, shape):\n",
    "        for (name, (i, j)) in self.facial_landmarks_idxs.items():\n",
    "            # Loop over the subset of facial landmarks, drawing the specific face part\n",
    "            for (x, y) in shape[i:j]:\n",
    "                cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "        return frame\n",
    "    \n",
    "    # Displays the overlay of the landmarks \n",
    "    def display_overlay(self, frame, shape, colors=None, alpha=0.75):\n",
    "        # Create two copies of the input image -- one for the\n",
    "        # overlay and one for the final output image\n",
    "        overlay = frame.copy()\n",
    "        output = frame.copy()\n",
    "\n",
    "        # If the colors list is None, initialize it with a unique\n",
    "        # color for each facial landmark region\n",
    "        if colors is None:\n",
    "            colors = [(19, 199, 109), (79, 76, 240), (230, 159, 23),\n",
    "                (168, 100, 168), (158, 163, 32),\n",
    "                (163, 38, 32), (180, 42, 220), (100, 150, 250)]\n",
    "\n",
    "        # Loop over the facial landmark regions individually\n",
    "        for (i, name) in enumerate(self.facial_landmarks_idxs.keys()):\n",
    "            # Grab the (x, y)-coordinates associated with the\n",
    "            # face landmark\n",
    "            (j, k) = self.facial_landmarks_idxs[name]\n",
    "            pts = shape[j:k]\n",
    "            if name == 'face':\n",
    "                hull = cv2.convexHull(pts)\n",
    "                cv2.drawContours(overlay, [hull], -1, colors[i], -1)\n",
    "\n",
    "        # Apply the transparent overlay\n",
    "        cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n",
    "\n",
    "        # Return the output image\n",
    "        return output\n",
    "    \n",
    "    def plot_rgb(self):\n",
    "        num_frames = range(len(self.red))\n",
    "        plt.plot(num_frames, self.red, color='red', label = 'red')\n",
    "        plt.plot(num_frames, self.green, color='green', label='green')\n",
    "        plt.plot(num_frames, self.blue, color='blue', label='blue')\n",
    "\n",
    "    def draw_aam(self):\n",
    "        return\n",
    "                \n",
    "# next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful init\n",
      "[INFO] camera sensor warming up...\n"
     ]
    }
   ],
   "source": [
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "    (\"nose\", (27, 36)),\n",
    "    (\"face\", (0, 26))\n",
    "])\n",
    "predictor_path = \"../detect-face-parts/shape_predictor_68_face_landmarks.dat\"\n",
    "fs = face_streamer(predictor_path)\n",
    "fs.stream(display_face_bb = True, display_landmarks = False, display_overlay = False)\n",
    "# del fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d71411458071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Graph RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-e85ee73b653a>\u001b[0m in \u001b[0;36mplot_rgb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Graph RGB\n",
    "fs.plot_rgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_face_detection(filename=None):\n",
    "    # To capture video from webcam.\n",
    "    if not _filename:\n",
    "        vs = VideoStream(src=0).start()\n",
    "    else:\n",
    "        # To use a video file as input \n",
    "        vs = FileVideoStream(filename).start()\n",
    "    time.sleep(2.0)\n",
    "\n",
    "    while True:\n",
    "        # grab frame and resize\n",
    "        frame = vs.read()\n",
    "        frame = imutils.resize(frame, width=400)\n",
    "        # grab the frame dimensions and convert it to a blob\n",
    "        (h, w) = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "            (300, 300), (104.0, 177.0, 123.0))\n",
    "        # pass the blob through the network and obtain the detections and\n",
    "        # predictions\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # extract the confidence (i.e., probability) associated with the\n",
    "            # prediction\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            # filter out weak detections by ensuring the `confidence` is\n",
    "            # greater than the minimum confidence\n",
    "            if confidence < confidence_threshold:\n",
    "                continue\n",
    "            # compute the (x, y)-coordinates of the bounding box for the\n",
    "            # object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # draw the bounding box of the face along with the associated\n",
    "            # probability\n",
    "            text = \"{:.2f}%\".format(confidence * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "            cv2.putText(frame, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            # do a bit of cleanup\n",
    "            cv2.destroyAllWindows()\n",
    "            vs.stop()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(_path):\n",
    "    img = cv2.imread(_path,1)\n",
    "    while True:\n",
    "        cv2.imshow('image',img)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            # do a bit of cleanup\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
